{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, csv, torch, scipy.io, torchvision.transforms, PIL.Image\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from mit_semseg.models import ModelBuilder, SegmentationModule\n",
    "from mit_semseg.utils import colorEncode\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import urllib.request\n",
    "from tqdm import tqdm\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import requests\n",
    "import os\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder is not empty, skipping weight download.\n"
     ]
    }
   ],
   "source": [
    "model_weights = 'pretrained_seg_model/ckpt/ade20k-resnet50dilated-ppm_deepsup/'\n",
    "\n",
    "if not os.path.exists(model_weights):\n",
    "    os.makedirs(model_weights)\n",
    "\n",
    "# Check if folder is empty\n",
    "if not os.listdir(model_weights):\n",
    "    print(\"Folder is empty, downloading weights...\")\n",
    "    # Define the download function\n",
    "    def download_weights(url, filename):\n",
    "        with tqdm(unit='B', unit_scale=True, unit_divisor=1024, miniters=1, desc=filename) as t:\n",
    "            urllib.request.urlretrieve(url, model_weights + filename, reporthook=lambda blocks, block_size, total_size: t.update(block_size))\n",
    "\n",
    "    # Download the encoder weights\n",
    "    encoder_url = 'http://sceneparsing.csail.mit.edu/model/pytorch/ade20k-resnet50dilated-ppm_deepsup/encoder_epoch_20.pth'\n",
    "    encoder_filename = 'encoder_epoch_20.pth'\n",
    "    download_weights(encoder_url, encoder_filename)\n",
    "\n",
    "    # Download the decoder weights\n",
    "    decoder_url = 'http://sceneparsing.csail.mit.edu/model/pytorch/ade20k-resnet50dilated-ppm_deepsup/decoder_epoch_20.pth'\n",
    "    decoder_filename = 'decoder_epoch_20.pth'\n",
    "    download_weights(decoder_url, decoder_filename)\n",
    "\n",
    "    print(\"Weights downloaded successfully!\")\n",
    "else:\n",
    "    print(\"Folder is not empty, skipping weight download.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = scipy.io.loadmat('pretrained_seg_model/data/color150.mat')['colors']\n",
    "names = {}\n",
    "with open('pretrained_seg_model/data/object150_info.csv') as f:\n",
    "    reader = csv.reader(f)\n",
    "    next(reader)\n",
    "    for row in reader:\n",
    "        names[int(row[0])] = row[5].split(\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights for net_encoder\n",
      "Loading weights for net_decoder\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SegmentationModule(\n",
       "  (encoder): ResnetDilated(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (bn1): SynchronizedBatchNorm2d(64, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
       "    (relu1): ReLU(inplace=True)\n",
       "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn2): SynchronizedBatchNorm2d(64, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
       "    (relu2): ReLU(inplace=True)\n",
       "    (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn3): SynchronizedBatchNorm2d(128, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
       "    (relu3): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): SynchronizedBatchNorm2d(64, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): SynchronizedBatchNorm2d(64, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): SynchronizedBatchNorm2d(256, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): SynchronizedBatchNorm2d(256, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): SynchronizedBatchNorm2d(64, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): SynchronizedBatchNorm2d(64, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): SynchronizedBatchNorm2d(256, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): SynchronizedBatchNorm2d(64, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): SynchronizedBatchNorm2d(64, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): SynchronizedBatchNorm2d(256, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): SynchronizedBatchNorm2d(128, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): SynchronizedBatchNorm2d(128, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): SynchronizedBatchNorm2d(512, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): SynchronizedBatchNorm2d(512, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): SynchronizedBatchNorm2d(128, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): SynchronizedBatchNorm2d(128, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): SynchronizedBatchNorm2d(512, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): SynchronizedBatchNorm2d(128, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): SynchronizedBatchNorm2d(128, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): SynchronizedBatchNorm2d(512, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): SynchronizedBatchNorm2d(128, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): SynchronizedBatchNorm2d(128, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): SynchronizedBatchNorm2d(512, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): SynchronizedBatchNorm2d(256, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): SynchronizedBatchNorm2d(256, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): SynchronizedBatchNorm2d(1024, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): SynchronizedBatchNorm2d(1024, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): SynchronizedBatchNorm2d(256, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn2): SynchronizedBatchNorm2d(256, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): SynchronizedBatchNorm2d(1024, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): SynchronizedBatchNorm2d(256, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn2): SynchronizedBatchNorm2d(256, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): SynchronizedBatchNorm2d(1024, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): SynchronizedBatchNorm2d(256, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn2): SynchronizedBatchNorm2d(256, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): SynchronizedBatchNorm2d(1024, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (4): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): SynchronizedBatchNorm2d(256, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn2): SynchronizedBatchNorm2d(256, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): SynchronizedBatchNorm2d(1024, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (5): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): SynchronizedBatchNorm2d(256, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn2): SynchronizedBatchNorm2d(256, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): SynchronizedBatchNorm2d(1024, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): SynchronizedBatchNorm2d(512, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn2): SynchronizedBatchNorm2d(512, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): SynchronizedBatchNorm2d(2048, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): SynchronizedBatchNorm2d(2048, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): SynchronizedBatchNorm2d(512, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)\n",
       "        (bn2): SynchronizedBatchNorm2d(512, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): SynchronizedBatchNorm2d(2048, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): SynchronizedBatchNorm2d(512, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)\n",
       "        (bn2): SynchronizedBatchNorm2d(512, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): SynchronizedBatchNorm2d(2048, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): PPMDeepsup(\n",
       "    (ppm): ModuleList(\n",
       "      (0): Sequential(\n",
       "        (0): AdaptiveAvgPool2d(output_size=1)\n",
       "        (1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (2): SynchronizedBatchNorm2d(512, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
       "        (3): ReLU(inplace=True)\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): AdaptiveAvgPool2d(output_size=2)\n",
       "        (1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (2): SynchronizedBatchNorm2d(512, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
       "        (3): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): AdaptiveAvgPool2d(output_size=3)\n",
       "        (1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (2): SynchronizedBatchNorm2d(512, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
       "        (3): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): AdaptiveAvgPool2d(output_size=6)\n",
       "        (1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (2): SynchronizedBatchNorm2d(512, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
       "        (3): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (cbr_deepsup): Sequential(\n",
       "      (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (1): SynchronizedBatchNorm2d(512, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (conv_last): Sequential(\n",
       "      (0): Conv2d(4096, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (1): SynchronizedBatchNorm2d(512, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Dropout2d(p=0.1, inplace=False)\n",
       "      (4): Conv2d(512, 150, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (conv_last_deepsup): Conv2d(512, 150, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (dropout_deepsup): Dropout2d(p=0.1, inplace=False)\n",
       "  )\n",
       "  (crit): NLLLoss()\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Network Builders\n",
    "net_encoder = ModelBuilder.build_encoder(\n",
    "    arch='resnet50dilated',\n",
    "    fc_dim=2048,\n",
    "    weights='pretrained_seg_model/ckpt/ade20k-resnet50dilated-ppm_deepsup/encoder_epoch_20.pth')\n",
    "net_decoder = ModelBuilder.build_decoder(\n",
    "    arch='ppm_deepsup',\n",
    "    fc_dim=2048,\n",
    "    num_class=150,\n",
    "    weights='pretrained_seg_model/ckpt/ade20k-resnet50dilated-ppm_deepsup/decoder_epoch_20.pth',\n",
    "    use_softmax=True)\n",
    "\n",
    "crit = torch.nn.NLLLoss(ignore_index=-1)\n",
    "segmentation_module = SegmentationModule(net_encoder, net_decoder, crit)\n",
    "segmentation_module.eval()\n",
    "segmentation_module.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_result(img, pred, index=None):\n",
    "    # filter prediction class if requested\n",
    "    if index is not None:\n",
    "        pred = pred.copy()\n",
    "        pred[pred != index] = -1\n",
    "        print(f'{names[index+1]}:')\n",
    "        \n",
    "    # colorize prediction\n",
    "    pred_color = colorEncode(pred, colors).astype(np.uint8)\n",
    "\n",
    "    # aggregate images and save\n",
    "    im_vis = np.concatenate((img, pred_color), axis=1)\n",
    "    display(Image.fromarray(im_vis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_to_arr(image, n):\n",
    "    '''\n",
    "        Takes the path to an image and n, the number of rows and columns in the grid\n",
    "        Returns an array of n**2, h/n x w/n images\n",
    "    '''\n",
    "    # Convert the image to a numpy array\n",
    "    image_array = np.array(image)\n",
    "    h, w, c = image_array.shape\n",
    "    # Split the image array into n**2 h/n x w/n sections\n",
    "    sections = []\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            section = image_array[i*h//n:(i+1)*h//n, j*w//n:(j+1)*w//n]\n",
    "            sections.append(section)\n",
    "    # Convert the sections list to a numpy array\n",
    "    sections_array = np.array(sections)\n",
    "    return sections_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_grid_pred(arr, n):\n",
    "    '''\n",
    "        Takes an array of predictions and n, the number of rows and columns in the grid\n",
    "        Returns an n x n grid of predictions\n",
    "    '''\n",
    "    h, w = arr.shape[:2]\n",
    "    grid_h, grid_w = n, n\n",
    "    sub_h, sub_w = h // grid_h, w // grid_w\n",
    "    grid = np.zeros((grid_h, grid_w, sub_h, sub_w) + arr.shape[2:], dtype=arr.dtype)\n",
    "    for i in range(grid_h):\n",
    "        for j in range(grid_w):\n",
    "            sub_arr = arr[i*sub_h:(i+1)*sub_h, j*sub_w:(j+1)*sub_w]\n",
    "            grid[i, j] = sub_arr\n",
    "    return grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_captcha_solution(input_img, output_list, n, look_for):\n",
    "    '''\n",
    "        Takes the path to an image, the output list from solve_3_x_3 or solve_4_x_4, the number of rows and columns in the grid, and the name of the object to look for\n",
    "    \n",
    "        Returns a visualization of the predictions for each square.\n",
    "    '''\n",
    "    array_img = img_to_arr(input_img, n)\n",
    "    fig, ax = plt.subplots(n, n, figsize=(8, 8))\n",
    "    fig.suptitle(f'Looking for {look_for}', fontsize=16)\n",
    "    for i, axi in enumerate(ax.flat):\n",
    "            axi.imshow(array_img[i])\n",
    "            if output_list[i]:\n",
    "                axi.add_patch(plt.Rectangle((0, 0), array_img[i].shape[1], array_img[i].shape[0], linewidth=15, edgecolor='g', facecolor='none'))\n",
    "            else:\n",
    "                axi.add_patch(plt.Rectangle((0, 0), array_img[i].shape[1], array_img[i].shape[0], linewidth=15, edgecolor='r', facecolor='none'))\n",
    "            axi.set(xticks=[], yticks=[])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_3_x_3(image, target_name, confidence, debug=False):\n",
    "    '''\n",
    "        Solves a 3x3 recaptcha image.\n",
    "        img_path: The path to the image to solve.\n",
    "        target_name: The name of the target class. E.g. \"stairs\" Target class should be in the 150 classes of the MIT dataset.\n",
    "        confidence: The confidence threshold. The function will append 'True' if the target class is within the top 'confidence' predictions.\n",
    "        debug: If True, will visualize the predictions for each square.\n",
    "\n",
    "        Returns \n",
    "            pred_df: A dataframe of the predictions for each square.\n",
    "            output_list: A list of booleans. True if the target class is in the top 'confidence' predictions for that square. False otherwise.\n",
    "    '''\n",
    "\n",
    "    # Define the transformation to apply to each image\n",
    "    pil_to_tensor = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406], # These are RGB mean+std values\n",
    "            std=[0.229, 0.224, 0.225])  # across a large photo dataset.\n",
    "    ])\n",
    "\n",
    "    # Define the output list to return\n",
    "    output_list = []\n",
    "    # Define the dictionary of predictions to return\n",
    "    pred_dict = {}\n",
    "\n",
    "    img_array = img_to_arr(image, 3)\n",
    "    # Iterate over each image in the array\n",
    "    enumerate(img_array)\n",
    "    for idx, img in enumerate(img_array):\n",
    "        # Convert the image to a tensor and apply the transformation\n",
    "        pil_image = Image.fromarray(img)\n",
    "        img_data = pil_to_tensor(pil_image)\n",
    "        singleton_batch = {'img_data': img_data[None].cpu()}\n",
    "\n",
    "        # Run the segmentation model on the image\n",
    "        with torch.no_grad():\n",
    "            scores = segmentation_module(singleton_batch, segSize=img_data.shape[1:])\n",
    "        _, pred = torch.max(scores, dim=1)\n",
    "        pred = pred.cpu()[0].numpy()\n",
    "\n",
    "        # Visualize the predictions\n",
    "        if debug:\n",
    "            visualize_result(img, pred)\n",
    "\n",
    "        # Check if the target class is present in the image\n",
    "        target_class = -1\n",
    "        for i in range(len(names)):\n",
    "            if target_name in names[i+1]:\n",
    "                target_class = i\n",
    "                break\n",
    "        if target_class == -1:\n",
    "            raise ValueError('Target class not found in the class names.')\n",
    "        \n",
    "        top_preds = np.argsort(np.bincount(pred.flatten()))[::-1][:confidence]\n",
    "        top_classes = [names[i+1] for i in top_preds]\n",
    "        is_present = target_class in top_preds\n",
    "        \n",
    "        output_list.append(is_present)\n",
    "        pred_dict[idx] = {f'Pred #{str(i+1)}': val for i, val in enumerate(top_classes)}\n",
    "        \n",
    "    pred_df = pd.DataFrame(pred_dict)\n",
    "    return output_list, pred_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_4x4(image, target_name, confidence):\n",
    "    '''\n",
    "        Solves a 4x4 recaptcha image.\n",
    "        img_path: The path to the image to solve.\n",
    "        target_name: The name of the target class. E.g. \"stairs\" Target class should be in the 150 classes of the MIT dataset.\n",
    "        confidence: The confidence threshold. The function will append 'True' if the target class is within the top 'confidence' predictions.\n",
    "        debug: If True, will visualize the predictions for each square.\n",
    "\n",
    "        Returns \n",
    "            pred_df: A dataframe of the predictions for each square.\n",
    "            output_list: A list of booleans. True if the target class is in the top 'confidence' predictions for that square. False otherwise.\n",
    "    '''\n",
    "\n",
    "    # Define the transformation to apply to the image\n",
    "    pil_to_tensor = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406], # These are RGB mean+std values\n",
    "            std=[0.229, 0.224, 0.225])  # across a large photo dataset.\n",
    "    ])\n",
    "\n",
    "    # Define the output list to return\n",
    "    output_list = []\n",
    "    # Define the dictionary of predictions to return\n",
    "    pred_dict = {}\n",
    "\n",
    "    # Convert the image to a numpy array\n",
    "    img_original = np.array(image)\n",
    "    # Convert the image to a tensor\n",
    "    img_data = pil_to_tensor(image)\n",
    "    # Create a singleton batch\n",
    "    singleton_batch = {'img_data': img_data[None].cpu()}\n",
    "    # Get the output size\n",
    "    output_size = img_data.shape[1:]\n",
    "\n",
    "    # Run the segmentation at the highest resolution.\n",
    "    with torch.no_grad():\n",
    "        scores = segmentation_module(singleton_batch, segSize=output_size)\n",
    "\n",
    "    # Get the predicted scores for each pixel\n",
    "    _, pred = torch.max(scores, dim=1)\n",
    "    pred = pred.cpu()[0].numpy()\n",
    "\n",
    "    # Create a grid of predictions\n",
    "    pred_grid = create_grid_pred(pred, 4)\n",
    "\n",
    "    for r in range(4):\n",
    "        for c in range(4):\n",
    "            # Get the prediction for the current square\n",
    "            pred = pred_grid[r][c]\n",
    "            # Check if the target class is present in the image\n",
    "            target_class = -1\n",
    "            for i in range(len(names)):\n",
    "                if target_name in names[i+1]:\n",
    "                    target_class = i\n",
    "                    break\n",
    "            if target_class == -1:\n",
    "                raise ValueError('Target class not found in the class names.')\n",
    "            \n",
    "            top_preds = np.argsort(np.bincount(pred.flatten()))[::-1][:confidence]\n",
    "            top_classes = [names[i+1] for i in top_preds]\n",
    "            is_present = target_class in top_preds\n",
    "            \n",
    "            output_list.append(is_present)\n",
    "            pred_dict[f'{r},{c}'] = top_classes\n",
    "\n",
    "    # Get maximum length of arrays in pred_dict\n",
    "    max_len = max(len(v) for v in pred_dict.values())\n",
    "    # Pad shorter arrays with NaN values\n",
    "    d_padded = {k: v + [np.nan]*(max_len - len(v)) for k, v in pred_dict.items()}\n",
    "    # Convert to dataframe\n",
    "    pred_df = pd.DataFrame.from_dict(d_padded)\n",
    "    return output_list, pred_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('captcha_type.pickle', 'rb') as file:\n",
    "    captcha_type_model = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_label(label):\n",
    "    class_names = ['letters', 'nonsegmentation', 'segmentation']\n",
    "    return class_names[label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/jamescavallo/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def parse_target(filename):\n",
    "    pattern = r\"(?:a )?([\\w\\s]+)(?:_\\d+)?(?:\\.(png|jpg|jpeg))?$\"\n",
    "    match = re.search(pattern, filename)\n",
    "    if match:\n",
    "        noun = match.group(1)\n",
    "        noun = WordNetLemmatizer().lemmatize(noun)\n",
    "        return noun\n",
    "    else:\n",
    "        print(\"No noun found in the file name.\")\n",
    "        return \"Not found\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_present(target):\n",
    "    return target in [val for sublist in names.values() for val in sublist]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Pipeline Begins Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(image, label, pred_type):\n",
    "    confidence =  10\n",
    "    target = parse_target(label)\n",
    "    print(\"Target\", target)\n",
    "    if not target_present(target):\n",
    "        print(target, \"is not found in our classes\")\n",
    "        return (False, False)\n",
    "    pil_image = image.convert('RGB')\n",
    "    \n",
    "    if pred_type == \"nonsegmentation\":\n",
    "        output_list, pred_df = solve_3_x_3(pil_image, target, confidence)\n",
    "        return output_list, pred_df\n",
    "    elif pred_type == \"segmentation\":\n",
    "        output_list, pred_df = solve_4x4(pil_image, target, confidence)\n",
    "        return output_list, pred_df\n",
    "    else:\n",
    "        print(\"Letters detected\")\n",
    "        return (False, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def scrape_image(driver, wait):\n",
    "\n",
    "    driver.switch_to.default_content()\n",
    "    challenge_iframe = wait.until(EC.frame_to_be_available_and_switch_to_it((By.CSS_SELECTOR, 'iframe[title=\"recaptcha challenge expires in two minutes\"]')))   \n",
    "    image = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, 'img[src*=\"api2/payload\"]')))\n",
    "\n",
    "    image_src = image.get_attribute('src')\n",
    "    class_name = image.get_attribute('class')\n",
    "\n",
    "\n",
    "    label = driver.find_element(By.TAG_NAME, 'strong').text\n",
    "\n",
    "    response = requests.get(image_src)\n",
    "\n",
    "    image_content = response.content\n",
    "\n",
    "        # Open the image from bytes\n",
    "    image = Image.open(BytesIO(image_content))\n",
    "\n",
    "    file_name = f\"{label}{random.randint(0, 10000000)}.png\"  # You can change the file extension to any image format you prefer\n",
    "    file_path = os.path.join(\"captchas/test/\", file_name)\n",
    "    image.save(file_path)\n",
    "    \n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drive_3x3(driver, image, label, pred_type, click_delay, wait):\n",
    "    output_list, pred_df = pipeline(image, label, pred_type) \n",
    "    print(\"original predictions\", output_list)\n",
    "\n",
    "    while (True in output_list):\n",
    "        elements = driver.find_elements(By.CLASS_NAME, 'rc-imageselect-tile')\n",
    "                \n",
    "        idx = 0\n",
    "        for element in elements:\n",
    "            if output_list[idx]:\n",
    "                element.click()\n",
    "                time.sleep(click_delay)\n",
    "            idx +=1\n",
    "        time.sleep(6.0) #wait for all new images to fade in and retry\n",
    "        new_image, new_label = scrape_image(driver, wait) #TODO Fix this bug\n",
    "        output_list, pred_df = pipeline(new_image, new_label, \"nonsegmentation\")\n",
    "        print(output_list)\n",
    "        print(\"One 3x3 Passthrough complete\") \n",
    "        \n",
    "\n",
    "    verify_button = driver.find_element(By.ID, 'recaptcha-verify-button')\n",
    "    verify_button.click()\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drive_4x4(driver, image, label, pred_type, click_delay):\n",
    "        #predict on our scraped captcha\n",
    "        output_list, pred_df = pipeline(image, label, pred_type) \n",
    "\n",
    "        # Find all elements with class name \"rc-imageselect-tile\"\n",
    "        elements = driver.find_elements(By.CLASS_NAME, 'rc-imageselect-tile')\n",
    "            \n",
    "        idx = 0\n",
    "        for element in elements:\n",
    "            if output_list[idx]:\n",
    "                element.click()\n",
    "                time.sleep(click_delay)\n",
    "            idx +=1\n",
    "\n",
    "        verify_button = driver.find_element(By.ID, 'recaptcha-verify-button')\n",
    "        verify_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Solve\n",
      "1/1 [==============================] - 0s 134ms/step\n",
      "4x4 segmentation detected\n",
      "Target motorcycle\n",
      "4x4 complete\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "4x4 segmentation detected\n",
      "Target crosswalk\n",
      "4x4 complete\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "4x4 segmentation detected\n",
      "Target crosswalk\n",
      "4x4 complete\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "4x4 segmentation detected\n",
      "Target crosswalk\n",
      "4x4 complete\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "4x4 segmentation detected\n",
      "Target crosswalk\n",
      "4x4 complete\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "4x4 segmentation detected\n",
      "Target crosswalk\n",
      "4x4 complete\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "4x4 segmentation detected\n",
      "Target crosswalk\n",
      "4x4 complete\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "4x4 segmentation detected\n",
      "Target crosswalk\n",
      "4x4 complete\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "4x4 segmentation detected\n",
      "Target crosswalk\n",
      "4x4 complete\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "4x4 segmentation detected\n",
      "Target crosswalk\n",
      "4x4 complete\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "4x4 segmentation detected\n",
      "Target crosswalk\n",
      "4x4 complete\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "4x4 segmentation detected\n",
      "Target crosswalk\n",
      "4x4 complete\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "4x4 segmentation detected\n",
      "Target crosswalk\n",
      "4x4 complete\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "4x4 segmentation detected\n",
      "Target crosswalk\n",
      "4x4 complete\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "4x4 segmentation detected\n",
      "Target crosswalk\n",
      "4x4 complete\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "4x4 segmentation detected\n",
      "Target crosswalk\n",
      "4x4 complete\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "4x4 segmentation detected\n",
      "Target crosswalk\n",
      "4x4 complete\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "4x4 segmentation detected\n",
      "Target crosswalk\n",
      "4x4 complete\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "4x4 segmentation detected\n",
      "Target crosswalk\n",
      "4x4 complete\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "4x4 segmentation detected\n",
      "Target crosswalk\n",
      "4x4 complete\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "4x4 segmentation detected\n",
      "Target crosswalk\n",
      "4x4 complete\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "4x4 segmentation detected\n",
      "Target crosswalk\n",
      "4x4 complete\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "4x4 segmentation detected\n",
      "Target crosswalk\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 37\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[39mif\u001b[39;00m pred_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39msegmentation\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m     36\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m4x4 segmentation detected\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 37\u001b[0m     drive_4x4(driver, image, label, pred_type, click_delay)\n\u001b[1;32m     38\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m4x4 complete\u001b[39m\u001b[39m\"\u001b[39m) \n\u001b[1;32m     39\u001b[0m \u001b[39melif\u001b[39;00m pred_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mnonsegmentation\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "Cell \u001b[0;32mIn[38], line 3\u001b[0m, in \u001b[0;36mdrive_4x4\u001b[0;34m(driver, image, label, pred_type, click_delay)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdrive_4x4\u001b[39m(driver, image, label, pred_type, click_delay):\n\u001b[1;32m      2\u001b[0m         \u001b[39m#predict on our scraped captcha\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m         output_list, pred_df \u001b[39m=\u001b[39m pipeline(image, label, pred_type) \n\u001b[1;32m      5\u001b[0m         \u001b[39m# Find all elements with class name \"rc-imageselect-tile\"\u001b[39;00m\n\u001b[1;32m      6\u001b[0m         elements \u001b[39m=\u001b[39m driver\u001b[39m.\u001b[39mfind_elements(By\u001b[39m.\u001b[39mCLASS_NAME, \u001b[39m'\u001b[39m\u001b[39mrc-imageselect-tile\u001b[39m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[35], line 14\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(image, label, pred_type)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[39mreturn\u001b[39;00m output_list, pred_df\n\u001b[1;32m     13\u001b[0m \u001b[39melif\u001b[39;00m pred_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39msegmentation\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m---> 14\u001b[0m     output_list, pred_df \u001b[39m=\u001b[39m solve_4x4(pil_image, target, confidence)\n\u001b[1;32m     15\u001b[0m     \u001b[39mreturn\u001b[39;00m output_list, pred_df\n\u001b[1;32m     16\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[30], line 38\u001b[0m, in \u001b[0;36msolve_4x4\u001b[0;34m(image, target_name, confidence)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[39m# Run the segmentation at the highest resolution.\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m---> 38\u001b[0m     scores \u001b[39m=\u001b[39m segmentation_module(singleton_batch, segSize\u001b[39m=\u001b[39;49moutput_size)\n\u001b[1;32m     40\u001b[0m \u001b[39m# Get the predicted scores for each pixel\u001b[39;00m\n\u001b[1;32m     41\u001b[0m _, pred \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmax(scores, dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/CAPTCHA_ML/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Desktop/CAPTCHA_ML/.venv/lib/python3.10/site-packages/mit_semseg/models/models.py:46\u001b[0m, in \u001b[0;36mSegmentationModule.forward\u001b[0;34m(self, feed_dict, segSize)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[39mreturn\u001b[39;00m loss, acc\n\u001b[1;32m     44\u001b[0m \u001b[39m# inference\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 46\u001b[0m     pred \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(feed_dict[\u001b[39m'\u001b[39;49m\u001b[39mimg_data\u001b[39;49m\u001b[39m'\u001b[39;49m], return_feature_maps\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m), segSize\u001b[39m=\u001b[39msegSize)\n\u001b[1;32m     47\u001b[0m     \u001b[39mreturn\u001b[39;00m pred\n",
      "File \u001b[0;32m~/Desktop/CAPTCHA_ML/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Desktop/CAPTCHA_ML/.venv/lib/python3.10/site-packages/mit_semseg/models/models.py:263\u001b[0m, in \u001b[0;36mResnetDilated.forward\u001b[0;34m(self, x, return_feature_maps)\u001b[0m\n\u001b[1;32m    261\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer1(x); conv_out\u001b[39m.\u001b[39mappend(x);\n\u001b[1;32m    262\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer2(x); conv_out\u001b[39m.\u001b[39mappend(x);\n\u001b[0;32m--> 263\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayer3(x); conv_out\u001b[39m.\u001b[39mappend(x);\n\u001b[1;32m    264\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer4(x); conv_out\u001b[39m.\u001b[39mappend(x);\n\u001b[1;32m    266\u001b[0m \u001b[39mif\u001b[39;00m return_feature_maps:\n",
      "File \u001b[0;32m~/Desktop/CAPTCHA_ML/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Desktop/CAPTCHA_ML/.venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/Desktop/CAPTCHA_ML/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Desktop/CAPTCHA_ML/.venv/lib/python3.10/site-packages/mit_semseg/models/resnet.py:83\u001b[0m, in \u001b[0;36mBottleneck.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     80\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn2(out)\n\u001b[1;32m     81\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(out)\n\u001b[0;32m---> 83\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv3(out)\n\u001b[1;32m     84\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn3(out)\n\u001b[1;32m     86\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdownsample \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Desktop/CAPTCHA_ML/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Desktop/CAPTCHA_ML/.venv/lib/python3.10/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/Desktop/CAPTCHA_ML/.venv/lib/python3.10/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "captcha_site = 'https://www.google.com/recaptcha/api2/demo'\n",
    "click_delay = .5\n",
    "solved = False\n",
    "\n",
    "# create a new instance of the Chrome browser\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# navigate to the website\n",
    "driver.get(captcha_site)\n",
    "\n",
    "# wait for reCAPTCHA iframe to load and switch to it\n",
    "wait = WebDriverWait(driver, 10)\n",
    "    # switch back to the main frame and wait for the \"recaptcha challenge expires in two minutes\" iframe to load\n",
    "recaptcha_iframe = wait.until(EC.frame_to_be_available_and_switch_to_it((By.CSS_SELECTOR, 'iframe[title=\"reCAPTCHA\"]')))\n",
    "\n",
    "    # wait for div#rc-anchor-container to load and click on div.recaptcha-checkbox-border\n",
    "recaptcha_wait = WebDriverWait(driver, 10)\n",
    "recaptcha = recaptcha_wait.until(EC.presence_of_element_located((By.ID, 'rc-anchor-container')))\n",
    "recaptcha.click()\n",
    "\n",
    "print(\"Starting Solve\")\n",
    "while(not solved):\n",
    "    # find the captcha image element and get its source and class name\n",
    "    image, label = scrape_image(driver, wait)\n",
    "\n",
    "    #convert it for predictions\n",
    "    pil_image = image.convert('RGB')\n",
    "    img_original = np.array(pil_image)\n",
    "    img = cv2.resize(img_original, (120, 120))\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "\n",
    "    captcha_type = captcha_type_model.predict(img)          \n",
    "    pred_type = decode_label(np.argmax(captcha_type))\n",
    "\n",
    "    if pred_type == \"segmentation\":\n",
    "        print(\"4x4 segmentation detected\")\n",
    "        drive_4x4(driver, image, label, pred_type, click_delay)\n",
    "        print(\"4x4 complete\") \n",
    "    elif pred_type == \"nonsegmentation\":\n",
    "        print(\"3x3 nonsegmentation detected\")\n",
    "        drive_3x3(driver, image, label, pred_type, click_delay, wait)\n",
    "        print(\"3x3 complete\") \n",
    "    else:\n",
    "        pass\n",
    "\n",
    "# switch back to the main frame and quit the browser\n",
    "driver.switch_to.default_content()\n",
    "driver.quit()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
